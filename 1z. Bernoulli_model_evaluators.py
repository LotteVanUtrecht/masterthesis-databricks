# Databricks notebook source
import numpy as np
import pandas as pd
import sys
import operator
import math
import pyspark.mllib 
import pyspark.sql.functions as f
from matplotlib import pyplot as plt
from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType, BooleanType, StructField, LongType, DateType,TimestampType, FloatType, ArrayType
from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel, LogisticRegression, LogisticRegressionModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, VectorIndexer
from pyspark.ml import Pipeline

# COMMAND ----------

market_test = spark.read.parquet("s3://rtl-databricks-datascience/lpater/processed_data/market_test.parquet/")
market_test.cache()

deal_ids = list(spark.read.parquet("s3://rtl-databricks-datascience/lpater/processed_data/bids_test.parquet/").groupBy("deal_id").count().orderBy('count', ascending=False).select("deal_id").toPandas()["deal_id"]) #create a list of deal_ids to select from, ordered by how common they are
summary_stats = spark.read.parquet("s3://rtl-databricks-datascience/lpater/processed_data/bids_test.parquet/").groupBy("deal_id").count().orderBy('count', ascending=False)

# COMMAND ----------

def get_metrics(deal_id,test_data=market_test):  
  ####In: 
  #A testing data set, as generated by data_prep() 
  #The deal_id you want to test a model for
  
  ####Out
  #The two sets of accuracies and are unders the PR curves are outputted 
  
  #import models
  model_lr = LogisticRegressionModel.load(f"/mnt/lotte/logistic_regression/{deal_id}/")
  model_trees = DecisionTreeClassificationModel.load(f"/mnt/lotte/decision_trees/{deal_id}/")
  
  #fit models
  predictions_lr = model_lr.transform(test_data.withColumnRenamed(deal_id,'label'))
  predictions_trees = model_trees.transform(test_data.withColumnRenamed(deal_id,'label'))

  #define evaluators
  evaluator_accuracy = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction",
                                                metricName="accuracy")
  evaluator_area = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="prediction",
                                                metricName="areaUnderPR")
  
  #get metrics
  lr_accuracy = evaluator_accuracy.evaluate(predictions_lr)
  lr_area = evaluator_area.evaluate(predictions_lr)
  trees_accuracy = evaluator_accuracy.evaluate(predictions_trees)
  trees_area = evaluator_area.evaluate(predictions_trees)
  
  #gather metrics
  metrics = [lr_accuracy,lr_area,trees_accuracy,trees_area]
  return metrics

metrics_udf = udf(lambda z: get_metrics(z), ArrayType(FloatType()))


# COMMAND ----------

summary_stats_list = [get_metrics(deal_id,market_test) for deal_id in deal_ids]

# COMMAND ----------

summary_stats_list
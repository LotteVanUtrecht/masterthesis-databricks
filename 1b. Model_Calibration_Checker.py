# Databricks notebook source
# MAGIC %md # Model Calibration Checker
# MAGIC 
# MAGIC This notebook checks whether models are well-calibrated, that is that they predict a similar number of bids on the test set as there were actual bids in the data. Note that this notebook outputs two metrics for this: the number of impressions for which the bid probability is above .5 (called bid_predictions) and the sums of all bid probabilities (called bid_probabilities). The second metric is more relevant, since that is how bids are being chosen later. This notebook needs models generated by Bernoulli_Model_Generator and test data generated by Custom_Fields_Processor.

# COMMAND ----------

import numpy as np
import pandas as pd
import sys
import operator
import math
import pyspark.mllib 
import pyspark.sql.functions as f
from matplotlib import pyplot as plt
from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType, BooleanType, StructField, LongType, DateType,TimestampType, FloatType
from pyspark.ml.classification import NaiveBayes, NaiveBayesModel, LogisticRegression, LogisticRegressionModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder
from pyspark.ml import Pipeline

# COMMAND ----------

def logistic_regression_calibration(test_data,deal_id):  
  ####In: 
  #A testing data set, as generated by data_prep() 
  #The deal_id you want to test a model for
  #NB: The model has to be already saved to the cloud 
  
  ####Out
  #A list containing:
  #1) the true # of bids in the test set
  #2) the sum of all bid probabilities for the test set
  #3) the # of times the model predicts a probability >.5
  
  #https://stackoverflow.com/questions/54354915/pyspark-aggregate-sum-vector-element-wise
  
  model = LogisticRegressionModel.load(f"/mnt/lotte/logistic_regression/{deal_id}/")
  predictions = model.transform(test_data.withColumnRenamed(deal_id,'label').select("label","features"))
  
  predictions = predictions.select("label",ith("probability", f.lit(1)),"prediction") #the ith function selects the probability of category 1, i.e. the probability of bidding
  col_names = ["true bids","bid probabilities","bid predictions"]
  predictions = predictions.toDF(*col_names)
  
  probabilities = predictions.agg(f.sum("true bids"),f.sum("bid probabilities"),f.sum("bid predictions")).collect()
  
  return probabilities

def ith_(v, i):
    try:
        return float(v[i])
    except ValueError:
        return None

ith = f.udf(ith_, DoubleType())

# COMMAND ----------

#Import data, load in cache for easy working
#bids_train = spark.read.parquet("s3://rtl-databricks-datascience/lpater/processed_data/market_train.parquet/")
market_test = spark.read.parquet("s3://rtl-databricks-datascience/lpater/processed_data/market_test.parquet/")
market_test.cache()

deal_ids = spark.read.parquet("s3://rtl-databricks-datascience/lpater/processed_data/bids_train.parquet/").select("deal_id").distinct() #maybe add test?
deal_ids_list = list(deal_ids.select("deal_id").toPandas()["deal_id"])

# COMMAND ----------

for deal_id in deal_ids_list:
  print([deal_id,logistic_regression_calibration(test_data=market_test,deal_id=deal_id)])